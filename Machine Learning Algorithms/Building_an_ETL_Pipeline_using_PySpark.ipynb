{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqY3DfOUNw9d1WVZHhtSN2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratikagithub/DS-Case-Studies/blob/main/Building_an_ETL_Pipeline_using_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An ETL (Extract, Transform, and Load) pipeline extracts data from sources, transforms it, and loads it into a storage system. It helps create clean, usable data formats for analysis. PySpark is ideal for building ETL pipelines for large-scale data processing. It offers distributed computing, high performance, and handles structured and unstructured data efficiently.\n",
        "\n",
        "The dataset we will be using for building an ETL Pipeline contains temperature-related data for various countries from 1961 to 2022. The columns include identifiers like ObjectId, Country, ISO2, and ISO3, along with year-wise temperature data such as F1961, F1962, etc., as floating-point values. Some columns contain missing values.\n",
        "\n",
        "We’ll develop an ETL Pipeline using PySpark to process this dataset to handle the following tasks:\n",
        "\n",
        "Extract: Load the dataset from the CSV file.\n",
        "\n",
        "Transform: Clean the data, handle missing values, and pivot year-wise temperature data for analysis.\n",
        "\n",
        "Load: Save the processed data into a new storage format (e.g., Parquet or a database).\n",
        "\n",
        "**Step 1: Setting Up the Environment & Initializing a PySpark Session**\n",
        "\n",
        "Ensure that PySpark is installed and set up. Run the following command to install PySpark if it’s not already installed:\n",
        "\n",
        "Initialize a PySpark session to enable interaction with the Spark framework:"
      ],
      "metadata": {
        "id": "bowyzVg0ER4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0drHvomuEz6n",
        "outputId": "c2bf87d0-4fdc-40d0-daf2-24a2d32527b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3ZYbKEKEENC8"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ETL Pipeline\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Extract – Load the Dataset**\n",
        "\n",
        "The next step is to load the dataset into a PySpark DataFrame:"
      ],
      "metadata": {
        "id": "lG686R16FC3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the CSV file into a Spark DataFrame\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "file_path = \"/content/temperature.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# display the schema and preview the data\n",
        "df.printSchema()\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iyWcOdqOFHHK",
        "outputId": "4f35cb2f-eb18-4136-8a06-851a359d8ffe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9194a54b-8e98-4a6f-a0c8-5d144fb76dd2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9194a54b-8e98-4a6f-a0c8-5d144fb76dd2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving temperature.csv to temperature.csv\n",
            "root\n",
            " |-- ObjectId: integer (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            " |-- ISO2: string (nullable = true)\n",
            " |-- ISO3: string (nullable = true)\n",
            " |-- F1961: double (nullable = true)\n",
            " |-- F1962: double (nullable = true)\n",
            " |-- F1963: double (nullable = true)\n",
            " |-- F1964: double (nullable = true)\n",
            " |-- F1965: double (nullable = true)\n",
            " |-- F1966: double (nullable = true)\n",
            " |-- F1967: double (nullable = true)\n",
            " |-- F1968: double (nullable = true)\n",
            " |-- F1969: double (nullable = true)\n",
            " |-- F1970: double (nullable = true)\n",
            " |-- F1971: double (nullable = true)\n",
            " |-- F1972: double (nullable = true)\n",
            " |-- F1973: double (nullable = true)\n",
            " |-- F1974: double (nullable = true)\n",
            " |-- F1975: double (nullable = true)\n",
            " |-- F1976: double (nullable = true)\n",
            " |-- F1977: double (nullable = true)\n",
            " |-- F1978: double (nullable = true)\n",
            " |-- F1979: double (nullable = true)\n",
            " |-- F1980: double (nullable = true)\n",
            " |-- F1981: double (nullable = true)\n",
            " |-- F1982: double (nullable = true)\n",
            " |-- F1983: double (nullable = true)\n",
            " |-- F1984: double (nullable = true)\n",
            " |-- F1985: double (nullable = true)\n",
            " |-- F1986: double (nullable = true)\n",
            " |-- F1987: double (nullable = true)\n",
            " |-- F1988: double (nullable = true)\n",
            " |-- F1989: double (nullable = true)\n",
            " |-- F1990: double (nullable = true)\n",
            " |-- F1991: double (nullable = true)\n",
            " |-- F1992: double (nullable = true)\n",
            " |-- F1993: double (nullable = true)\n",
            " |-- F1994: double (nullable = true)\n",
            " |-- F1995: double (nullable = true)\n",
            " |-- F1996: double (nullable = true)\n",
            " |-- F1997: double (nullable = true)\n",
            " |-- F1998: double (nullable = true)\n",
            " |-- F1999: double (nullable = true)\n",
            " |-- F2000: double (nullable = true)\n",
            " |-- F2001: double (nullable = true)\n",
            " |-- F2002: double (nullable = true)\n",
            " |-- F2003: double (nullable = true)\n",
            " |-- F2004: double (nullable = true)\n",
            " |-- F2005: double (nullable = true)\n",
            " |-- F2006: double (nullable = true)\n",
            " |-- F2007: double (nullable = true)\n",
            " |-- F2008: double (nullable = true)\n",
            " |-- F2009: double (nullable = true)\n",
            " |-- F2010: double (nullable = true)\n",
            " |-- F2011: double (nullable = true)\n",
            " |-- F2012: double (nullable = true)\n",
            " |-- F2013: double (nullable = true)\n",
            " |-- F2014: double (nullable = true)\n",
            " |-- F2015: double (nullable = true)\n",
            " |-- F2016: double (nullable = true)\n",
            " |-- F2017: double (nullable = true)\n",
            " |-- F2018: double (nullable = true)\n",
            " |-- F2019: double (nullable = true)\n",
            " |-- F2020: double (nullable = true)\n",
            " |-- F2021: double (nullable = true)\n",
            " |-- F2022: double (nullable = true)\n",
            "\n",
            "+--------+--------------------+----+----+------+------+------+------+------+-----+------+------+------+------+------+------+------+------+------+------+------+------+-----+------+------+------+------+------+------+------+------+-----+------+-----+------+------+------+-----+------+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
            "|ObjectId|             Country|ISO2|ISO3| F1961| F1962| F1963| F1964| F1965|F1966| F1967| F1968| F1969| F1970| F1971| F1972| F1973| F1974| F1975| F1976| F1977| F1978|F1979| F1980| F1981| F1982| F1983| F1984| F1985| F1986| F1987|F1988| F1989|F1990| F1991| F1992| F1993|F1994| F1995| F1996|F1997|F1998|F1999|F2000|F2001|F2002|F2003|F2004|F2005|F2006|F2007|F2008|F2009|F2010|F2011|F2012|F2013|F2014|F2015|F2016|F2017|F2018|F2019|F2020|F2021|F2022|\n",
            "+--------+--------------------+----+----+------+------+------+------+------+-----+------+------+------+------+------+------+------+------+------+------+------+------+-----+------+------+------+------+------+------+------+------+-----+------+-----+------+------+------+-----+------+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
            "|       1|Afghanistan, Isla...|  AF| AFG|-0.113|-0.164| 0.847|-0.764|-0.244|0.226|-0.371|-0.423|-0.539| 0.813| 0.619|-1.124| 0.232|-0.489|-0.445|-0.286| 0.513| 0.129|0.361|   0.6| 0.483|-0.346| 0.164| 0.145| 0.283|-0.141| 0.391|0.919|-0.205| 0.73|-0.168|-0.294|  0.22| 0.43| 0.359|-0.116|0.471|0.675|1.198|0.993|1.311|1.365|0.587|1.373|0.401| 1.72|0.675|0.704|0.895|1.613|1.397|0.223|1.281|0.456|1.093|1.555| 1.54|1.544| 0.91|0.498|1.327|2.012|\n",
            "|       2|             Albania|  AL| ALB| 0.627| 0.326| 0.075|-0.166|-0.388|0.559|-0.074| 0.081|-0.013|-0.106|-0.195|-0.069|-0.288|-0.139|-0.211|-0.683| 0.545|-0.814|0.203|-0.414|-0.351| 0.173|-0.128| -0.27|-0.103| 0.569|-0.106| 0.37|-0.066|0.795|-0.269| 0.106| 0.076| 1.33|-0.172|-0.038|0.075|0.795| 0.67|1.065|1.532|0.492| 0.97|0.444|0.189|0.345|1.316|0.978| 0.91|1.191|1.055|1.487|1.333|1.198|1.569|1.464|1.121|2.028|1.675|1.498|1.536|1.518|\n",
            "|       3|             Algeria|  DZ| DZA| 0.164| 0.114| 0.077|  0.25|  -0.1|0.433|-0.026|-0.067| 0.291| 0.116|-0.385|-0.348|-0.015|-0.503|-0.539|-0.782| 0.504| 0.012|0.654| 0.232| 0.215| 0.399|  0.56|-0.004| 0.508| 0.296| 0.975|1.304| 0.386|1.266| 0.031|-0.312| 0.552|0.732| 0.595| 0.846|1.059|1.109|1.476| 0.82|1.856|1.258|1.585|0.988|1.264|1.395| 1.22|1.185|0.945|2.265|1.398|1.147|1.192| 1.69|1.121|1.757|1.512| 1.21|1.115|1.926| 2.33|1.688|\n",
            "|       4|      American Samoa|  AS| ASM| 0.079|-0.042| 0.169| -0.14|-0.562|0.181|-0.368|-0.187| 0.132|-0.047|-0.477|-0.067|  0.33|-0.308|-0.118|-0.177| 0.156| 0.092|0.341|  0.35| 0.179|  0.28| 0.313| 0.277| 0.256| 0.394| 0.354|0.509| 0.143|0.497| 0.641| 0.344|-0.069|0.189| 0.755| 0.784| NULL| NULL|0.242|0.626|0.904|1.152|0.716|0.191|0.801|0.403|1.032| 0.67| NULL|1.311|0.854|0.924|1.257| 1.17|1.009|1.539|1.435|1.189|1.539| 1.43|1.268|1.256|\n",
            "|       5|Andorra, Principa...|  AD| AND| 0.736| 0.112|-0.752| 0.308| -0.49|0.415| 0.637| 0.018|-0.137| 0.121|-0.326|-0.499| 0.025|-0.371| 0.246|-0.045|-0.093|-0.163|0.058|-0.188| 0.178| 1.044| 0.859|-0.157| 0.059| 0.387| 0.397|0.883| 1.162|1.736| 0.231| 0.386| 0.174|1.508| 1.279|  0.57|1.788|1.018|1.055| 1.05| 1.48|0.835|1.949|0.936|0.851|1.485|1.024|0.946|1.413|0.471|1.677|1.265|0.831|1.946| 1.69| 1.99|1.925|1.919|1.964|2.562|1.533|3.243|\n",
            "+--------+--------------------+----+----+------+------+------+------+------+-----+------+------+------+------+------+------+------+------+------+------+------+------+-----+------+------+------+------+------+------+------+------+-----+------+-----+------+------+------+-----+------+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "n PySpark, we are loading a CSV file into a distributed DataFrame, which is similar to using pandas.read_csv() to load data into a Pandas DataFrame. However, unlike Pandas, which uses memory and runs on a single machine, PySpark handles large datasets distributed across a cluster. The methods df.printSchema() and df.show(5) provide insights into the schema and preview the data, comparable to df.info() and df.head() in Pandas, but designed for scalable data exploration on big data workloads."
      ],
      "metadata": {
        "id": "QXILsu5AFZqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Transform – Clean and Process the Data**\n",
        "\n",
        "All datasets require different types of cleaning and processing steps. In this data, we will replace missing values in important columns like ISO2 or impute missing temperature values:"
      ],
      "metadata": {
        "id": "ox9VatyhFbx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill missing values for country codes\n",
        "df = df.fillna({\"ISO2\": \"Unknown\"})\n",
        "\n",
        "# drop rows where all temperature values are null\n",
        "temperature_columns = [col for col in df.columns if col.startswith('F')]\n",
        "df = df.dropna(subset=temperature_columns, how=\"all\")"
      ],
      "metadata": {
        "id": "dPirYe2jFfcW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will transform the dataset to have “Year” as a single column and its temperature value:"
      ],
      "metadata": {
        "id": "sRnswN-OFh5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# reshape temperature data to have 'Year' and 'Temperature' columns\n",
        "df_pivot = df.selectExpr(\n",
        "    \"ObjectId\", \"Country\", \"ISO3\",\n",
        "    \"stack(62, \" +\n",
        "    \",\".join([f\"'F{1961 + i}', F{1961 + i}\" for i in range(62)]) +\n",
        "    \") as (Year, Temperature)\"\n",
        ")\n",
        "\n",
        "# convert 'Year' column to integer\n",
        "df_pivot = df_pivot.withColumn(\"Year\", expr(\"int(substring(Year, 2, 4))\"))\n",
        "df_pivot.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eMdQA8tFkQE",
        "outputId": "943711bc-6ae1-4e4e-a065-4e3002b3d3ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+----+----+-----------+\n",
            "|ObjectId|             Country|ISO3|Year|Temperature|\n",
            "+--------+--------------------+----+----+-----------+\n",
            "|       1|Afghanistan, Isla...| AFG|1961|     -0.113|\n",
            "|       1|Afghanistan, Isla...| AFG|1962|     -0.164|\n",
            "|       1|Afghanistan, Isla...| AFG|1963|      0.847|\n",
            "|       1|Afghanistan, Isla...| AFG|1964|     -0.764|\n",
            "|       1|Afghanistan, Isla...| AFG|1965|     -0.244|\n",
            "+--------+--------------------+----+----+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Load – Save the Processed Data**\n",
        "\n",
        "After completing all the processing steps, you save the transformed data to a Parquet file for efficient storage and querying:"
      ],
      "metadata": {
        "id": "zqQfb-ZLFnOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"/processed_temperature.parquet\"\n",
        "df_pivot.write.mode(\"overwrite\").parquet(output_path)"
      ],
      "metadata": {
        "id": "alYImZJMFqwb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This operation saves the transformed DataFrame as a Parquet file, which optimizes it for storage and querying in a distributed environment.\n",
        "\n",
        "We can load the saved Parquet file to ensure the data was correctly saved:"
      ],
      "metadata": {
        "id": "M5W_XYGMFtA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the saved parquet file\n",
        "processed_df = spark.read.parquet(output_path)\n",
        "processed_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n-973R7Fuxh",
        "outputId": "c19a5099-8b38-422b-b24e-c58cf248a319"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+----+----+-----------+\n",
            "|ObjectId|             Country|ISO3|Year|Temperature|\n",
            "+--------+--------------------+----+----+-----------+\n",
            "|       1|Afghanistan, Isla...| AFG|1961|     -0.113|\n",
            "|       1|Afghanistan, Isla...| AFG|1962|     -0.164|\n",
            "|       1|Afghanistan, Isla...| AFG|1963|      0.847|\n",
            "|       1|Afghanistan, Isla...| AFG|1964|     -0.764|\n",
            "|       1|Afghanistan, Isla...| AFG|1965|     -0.244|\n",
            "+--------+--------------------+----+----+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}